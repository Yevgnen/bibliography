#+title: Paper notes
#+date: <2019-12-08 Sun>
#+author: Yevgnen Koh

* Han, X., Zhu, H., Yu, P., Wang, Z., Yao, Y., Liu, Z., & Sun, M. (2018): FewRel: a large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation
:PROPERTIES:
:Custom_ID: han-2018-fewrel
:END:

** 关系抽取

- Kernel Methods
- Embedding Methods
- Neural Methods
- Distant Supervision

** Few-Shot Learning

- Transfer Learning Methods
- Metric Methods
- Meta-Learning Methods

** 主要贡献

1. 将关系抽取定义成一个Few-Shot Learning任务，并提出了一个大型有监督few-shot关系抽取数据集（同一个关系的表达比较丰富）．

2. 对比多个SOTA Few-Shot Learning的关系抽取模型．

3. 用上述的模型在提出的数据集上进行评估．

** 训练策略

将已有模型与Few-Shot Learning结合的方法：

1. 微调

   在训练集上学习分类所有关系，然后在支撑集上微调．

2. k-NN

其它Few-Shot Learning方法：

1. Meta Network
2. GNN
3. SNAIL
4. Prototypical Network

** 结论

结合Few-Shot Learning的方法总是比直接使用编码器加微调或k-NN的方法要好．

* Gao, T., Han, X., Zhu, H., Liu, Z., Li, P., Sun, M., & Zhou, J. (2019): FewRel 2.0: towards more challenging few-shot relation classification
:PROPERTIES:
:Custom_ID: gao-etal-2019-fewrel-2
:END:

** 关系抽取里Few-Shot Learning的两个挑战

1. Few-shot domain adaptation (few-shot DA)

   评估Few-Shot模型的领域迁移能力．

2. Few-shot none-of-the-above detection (few-shot NOTA)

相对few-shot NOTA来说，few-shot DA目前的难度更大．

** Few-Shot Domain Adaptation

除了语料的形态与语法不一致之外，迁移过程里的两个语料库所涉及的关系集合也是不一样的．

对于few-shot DA，使用的训练语料与FewRel 1.0一样，但测试集使用新的不同领域的测试集（一个医疗领域的数据集，一个SemEval-2010 task8的数据集）．

*** 已有模型

- Subspace Mapping
- Finding Domain-Invariant Spaces
- Feature Augmentation
- Minimax Estimator
- Adversarial Training （目前比较有效）

** Few-Shot None-of-the Above Detection

NOTA通常会被当成额外的一类，识别它的难点在于由于评测的时候关系集并不是固定的，所以NOTA关系每次需要覆盖不同的语义空间．

在测试阶段所有的queries都来源于测试集，但是模型可以从训练集中采样数据作为NOTA关系的支撑集．评测使用的是FewRel 1.0的测试集而不是few-shot DA里的新测试集．

*** NOTA rate

测试集里NOTA queries所占的比例．

*** 已有模型

1. 直接把NOTA关系作为新的一类进行(\(N\) + 1)-way \(K\)-shot learning（在\(N\)类关系外的数据为NOTA采样作为支撑集）．

   效果可能不会太好，因为NOTA类的支撑集属于不同的其它关系，在特征空间里的分布比较离散．

2. BERT-PAIR

   将query与支撑集里的每一条数据进行配对拼接输入到BERT里，然后让模型输出一个二维向量分别计算这N类关系的概率与NOTA关系的概率，依旧使用交叉熵进行训练．

* Cui, L., & Zhang, Yue (2019): Hierarchically-refined label attention network for sequence labeling
:PROPERTIES:
:Custom_ID: cui-2019-bilstm-lan
:END:

Bi-LSTM-CRF并不是时时比Bi-LSTM(-softmax)好．一个可能的原因是深层句子的编码表示可能使得模型能够隐式地捕捉到长距离标签依赖．另外带CFR层可能受限于Markov假设并且解码速度受到影响．

** Bi-LSTM-LAN

1. 没有Markov假设与CRF层，输出的搜索空间巨大，指数级别．

2. 结合每个词与它标签的边缘概率建模，并且层级堆叠．

3. 模型对标签表示做attention并且与词表示一同作为隐层的输入．

4. 与Bi-LSTM的区别是：单层模型时两者等价；多层模型时，Bi-LSTM只堆叠编码器，而Bi-LSTM-LAN相当于同时堆叠softmax层．

6. 本文着重研究指数级标签序列的编码（长距离标签依赖），并且应该是首次使用层级attention网络对标签空间建模．

*** 结构

1. Bi-LSTM Encoding Sublayer

   Bi-LSTM隐层大小需要和标签表示层的大小一样．

2. Label-Attention Inference Sublayer

   #+begin_src latex :results drawer :exports results
   \begin{eqnarray}
     H^{l} & = &  \text{attention}(Q, K, V) = \alpha V \nonumber \\
     \alpha & = & \text{softmax}(\frac{QK^{T}}{\sqrt{d_{h}}}) \nonumber
   \end{eqnarray}
   #+end_src

   除了使用标准的attention之外，还可以使用multi-head attention来同时捕捉多个标签分布．

** 实验

1. 一般来说Bi-LSTM-LAN都使用两层以上，但即使只使用一层，它也比Bi-LSTM稍好，原因在于使用了标签编码．

2. 从和Bi-LSTM与Bi-LSTM-CRF的对比来看，模型的结构比模型的深度对效果的提升更重要．

3. 训练前期Bi-LSTM-LAN收敛速度比Bi-LSTM-CRF要慢一些，可能因为模型更复杂．

4. 标签表示会随着训练过程聚焦成簇．

5. Bi-LSTM学习长距离依赖相对困难，而Bi-LSTM-CRF虽然没这个问题，但对全局信息的捕捉有所欠缺．

* Shang, J., Liu, J., Jiang, M., Ren, X., Voss, C. R., & Han, J. (2017): Automated phrase mining from massive text corpora
:PROPERTIES:
:Custom_ID: shang-2017-autophrase
:END:

** 概述

1. 已有方法依赖于复杂的语言学分析器．

2. 已有的SOTA并非全自动化．

#+begin_quote
An ideal /Automated Phrase Mining/ method is supposed to be domain-independent, with minimal human effort or reliance on linguistic analyzers.
#+end_quote

为了减少人工标注和进一步提高效果，使用了这两种方式：

1. Robust Positive-Only Distant Training

   利用可用知识库里的high quality phrases标注正样本和domain corpora标注负样本并建立多个独立的分类器．通过这些分类器结果的聚合减少负样本的噪声．

2. POS-Guided Phrase Segmentation

   Domain independence与lingustic knowledge之间的存在着领域迁移泛化与精度矛盾．作为折中可使用一个predictive distribution-trained POS tagger来提高性能．POS tags作为一种比较浅层的语法信息，可以帮助phrasal segmentation model去定位phrase的边界．

因此本文的唯一假设就是：

1. The availability of a general knowledge base.

2. Together with a predictive distribution-trained POS tagger.

** 主要贡献

1. 研究Automated Phrase Mining的主要挑战．

2. 提出一种robust positive-only distant training method来进行短语质量估计，以最小化人工干预．

3. 当POS tagger可用的时候，利用一种phrasal segmentation model来提高效果．

4. 证明AutoPhrase的鲁棒性与有效性．

** 回顾

*** 相关工作

- Keyphrase Extraction
- Automatic Term Recognition
- Text Indexing Algorithms
  + Supervised Noun Phrase Chunking Techniques
  + Dependency Parsing
- Phrase Quality Estimation
- Data-driven approaches

  需要解决的问题是：

  - Candidate Generation
  - Quality Estimation

所有这些方法依然依赖于人工．

** 预备知识

目标是开发一个自动短语挖掘的方法从大规模文档库里抽取高质量短语而无需人工标注，只需要少量的浅层语言学分析．

- 输入：语料库、知识库．
- 输出：质量从高到低排序的短语列表．

短语质量评估参考cite:liu-2015-segphrase ．主要包括4个要素：

- Popularity: 出现足够的频次．
- Concordance：出现的概率要高于偶然出现．
- Informativeness：指示特定的话题或概念．
- Completeness：在特定的上下文环境可以解释作一个完整的语义单元．

AutoPhrase会根据正负样本在POS-guided phrasal segmentation前后对短语质量进行两次评估．

*** 流程

#+attr_html: :width 800px
#+attr_latex: :width 12cm
[[file:images/Shang,_J.,_Liu,_J.,_Jiang,_M.,_Ren,_X.,_Voss,_C._R.,_&_Han,_J._(2017):_Automated_phrase_mining_from_massive_text_corpora/2019-12-03_12-48-23_autophrase.png]]

**** 第一阶段

根据频率和长度的阈值在语料库里筛选出n-grams候选词，通过phrase quality estimator对其质量（主要是concordance与informativeness）进行评估．这个estimator能过统计学习所得，并独立于POS tags．

**** 第二阶段

通过phrasal segmentation为句子寻找最优切分．根据rectified frequency（这个频率表示一个短语能成完整语义单元的频率）重新计算统计特征，进行phrase quality re-estimation．这样phrase quality estimator同时也能对completeness进行评估．
** Methodology

*** Robust Positive-Only Distant Training

**** Label Pools

公共知识库的标题、关键字与内部链接等包含大量的高质量短语．利用这些短语构成positive pool．而从给定语料库里挖掘出来无法在知识库里匹配的候选短语构成一个大而带噪声的negative pool．

***** Noise Reduction

注意negative pool里可能有高质量短语，它们只是无法与已有的知识匹配．因些不能直接在带噪声的label pool里训练一个分类器．

#+attr_html: :width 800px
#+attr_latex: :width 8cm
[[file:images/Shang,_J.,_Liu,_J.,_Jiang,_M.,_Ren,_X.,_Voss,_C._R.,_&_Han,_J._(2017):_Automated_phrase_mining_from_massive_text_corpora/noise_reduction.png]]

作者使用不同的采样数据集(perturbed training set)，训练\(T\)个独立的训练器并将结果平均并融合．假定negative pool里high quality phrases的比例是10%，且挑选的特征能区分高质量与低质量短语，这种融合方式的经验错误\(p\)应该也接近这个数字．

短语质量评分phrase quality score可以表示为将这个短语预测为高质量短语的决策树的数量的比例．

*** POS-Guided Phrasal Segmentation

短语分割要解决的是completeness的刻画，主要通过字符串匹配找到语料库的所有提及并对它们出现的频率作出调整．POS-guided phrasal segmentation是一种考虑上下文的方法．

**** Phrase Quality Estimator

Given a phrase candidate \(w_{1}w_{2}\dots w_{n}\), its phrase quality is

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  Q(w_{1}w_{2}\dots w_{n}) = p(\lceil w_{1}w_{2}\dots w_{n} \rfloor | w_{1}w_{2}\dots w_{n}) \in [0, 1], \nonumber
\end{eqnarray}
#+end_src

where \(\lceil w_{1}w_{2}\dots w_{n} \rfloor\) refers to the event that these words constitute a phrase. \(Q(\cdot)\), also known as the phrase quality estimator, is initially learned from data based on statistical features.

**** POS quality score

POS quality score表示它对应的单词序列能成为一个完整语义单位的条件概率．可以看作是用于对短语的一种补偿得分

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  T(t_{[l, r)}) = p(\lceil w_{l} \dots w_{r} \rfloor | t) \in [0, 1]. \nonumber
\end{eqnarray}
#+end_src

作者采用了一种特殊的形式

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  T(t_{[l, r)}) = (1 - \delta(t_{b_{r} - 1}, t_{b_{r}})) \times \prod_{j = l + 1}^{r - 1} \delta(t_{j - 1}, t_{j}), \nonumber
\end{eqnarray}
#+end_src

其中\(\delta (t_{x}, t_{y})\)是文档集合某一个短语里POS tag \(t_{x}\)出现在POS tag\(t_{y}\)前面的概率．

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \delta(t_{x}, t_{y}) = p(\lceil \dots w_{1} w_{2} \dots \rfloor | \Omega, \text{tag}(w_{1}) = t_{x} \land \text{tag}(w_{2}) = t_{y}). \nonumber
\end{eqnarray}
#+end_src

POS quality score是length penalty model cite:liu-2015-segphrase 的一种推广．

\(\delta(t_{x}, t_{y})\)的计算依赖于文档是怎样分割成短语的，由随机初始化并在短语分割的过程里学习所得．

**** POS-Guided Phrasal Segmentation Model

给定phrase quality \(Q(\cdot)\)与POS quality \(T(\cdot)\)，可以定义PGPS模型．POS tagged sequence \(\Omega\)与分割边界序列\(B = \{b_{1}, b_{2}, \dots, b_{m + 1}\}\)可以作如下分解

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  p(\Omega, B) = \prod_{i}^{m} p(b_{i + 1} \lceil w_{[b_{i}, b_{i + 1})} \rfloor) | b_{i}, t), \nonumber
\end{eqnarray}
#+end_src

其中\(p(b_{i + 1} \lceil w_{[b_{i}, b_{i + 1}} \rfloor) | b_{i}, t)\)是观测到给定前一个边界索引\(b_{i}\)与整个POS tag序列\(t\)的情况下，观测到单词序列\(w_{[b_{i}, b_{i + 1})}\)作为第\(i\)个分割片段的概率．

为了简化问题，假定这些分割片段是依次生成的．给定一个POS tag序列\(t\)与起始索引\(b_{i}\)，生成过程如下：

1. 根据POS quality，生成结束索引\(b_{i + 1}\)：

   #+begin_src latex :results drawer :exports results
   \begin{eqnarray}
     p(b_{i + 1}| b_{i}, t) = T(t_{[b_{i}, b_{i + 1})}). \nonumber
   \end{eqnarray}
   #+end_src

2. 给定结束索引\(b_{i}\)和\(b_{i + 1}\)，利用所有长度为\(b_{i + 1} - b_{i}\)分割片段上的多项分布生成单词序列\(w_{[b_{i}, b_{i + 1})}\)：

   #+begin_src latex :results drawer :exports results
   \begin{eqnarray}
     p(w_{[b_{i}, b_{i + 1})}| b_{i}, b_{i + 1}) = p(w_{[b_{i}, b_{i + 1})}| b_{i} - b_{i + 1}) \doteq \theta(w_{[b_{i}, b_{i + 1})}). \nonumber
   \end{eqnarray}
   #+end_src

3. 根据phrase quality生成表示\(w_{[b_{i}, b_{i + 1})}\)能成否为一个quality segment的指标：

   #+begin_src latex :results drawer :exports results
   \begin{eqnarray}
     p(\lceil w_{[b_{i}, b_{i + 1})} \rfloor | w_{[b_{i}, b_{i + 1})}) = Q(w_{[b_{i}, b_{i + 1})}). \nonumber
   \end{eqnarray}
   #+end_src

从而得到概率分解：

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \begin{aligned}
    & p(b_{i + 1} \lceil w_{[b_{i}, b_{i + 1})} \rfloor) | b_{i}, t) \\
    = & p(b_{i + 1}| b_{i}, t)p(w_{[b_{i}, b_{i + 1})}| b_{i}, b_{i + 1})p(\lceil w_{[b_{i}, b_{i + 1})} \rfloor | w_{[b_{i}, b_{i + 1})}) \\
    = & T(t_{[b_{i}, b_{i + 1})})\theta(w_{[b_{i}, b_{i + 1})})Q(w_{[b_{i}, b_{i + 1})}) \\
  \end{aligned} \nonumber
\end{eqnarray}
#+end_src

有三个子问题需要解决：

1. Learn \(\theta_{u}\) for each word and phrase candidate \(u\).
2. Learn \(\delta_{t_{x}, t_{y}}\) for every POS tag pair.
3. Infer \(B\) when \(\theta_{u}\) and \(\delta(t_{x}, t_{y})\) are fixed.

通过极大化joint log likelihood

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \log p(\Omega, B) = \sum_{i}^{m} \log p(b_{i + 1} \lceil w_{[b_{i}, b_{i + 1})} \rfloor) | b_{i}, t) \nonumber
\end{eqnarray}
#+end_src

求解．

给定\(\theta_{u}\)和\(\delta(t_{x}, t_{y})\)，通过动态规划求解最优分割．


** 实验流程

1. 加载停用词表．

2. 加载训练语料．

   - 构建词表，记下总token数与总单词token数．其中token已经被转换成数字．

   - 记下每个token的上下文语法特征、词的大小写特征．

   - 计算每个token的IDF．

   - 构建POS tags词表，与POS tags序列（只对单词token序列构建）．

3. 对训练语料进行句子切分．

4. 频繁项挖掘，输出挖掘结果pattens的ranking列表．

5. 对每个pattern进行特征提取，并是考虑的是项的每个出现位置上的总体统计特征．

6. 构造正负样本集．例如要构造的集合类型为DPDN的时候，需要quality phrases的词表与全部的词表（如wiki_all）；而构建 的集合类型为EPEN的时候，则需要两个expert annotated labels的词表．此步构建出来的是频繁项的一个子集．

7. 构造分类正负样本的分类器，并对训练集进行预测．

* Shang, J., Liu, L., Ren, X., Gu, X., Ren, T., & Han, J. (2018): Learning named entity tagger using domain-specific dictionary
:PROPERTIES:
:Custom_ID: shang-2018-autoner
:END:

** 动机

1. 深度学习虽然减少了人工特征的需要，但依然需要大量有标注的数据．而远程监督需要减少了标注数据的需求，但产生的标签带有噪声．

   本文提出了两个模型来处理带噪声的基于词典的远程监督数据．

2. 远程监督通过一些诸如规则匹配的方法来处理实体块识别的问题，这些方法通常会导致FN较高．

   本文从语料库挖掘出高质量的短语，并将之标识为“未知”类型．而每个实体块可以带有多个类型标签．

** 主要贡献

1. 为远程监督训练提出一种Tie or Break标注方式．

2. 修改CRF层提出Fuazzy-LSTM-CRF以支持多标签的标注方式．

3. 探索了一些调整远程监督NER性能的方法，如挖掘高质量短语并用来减少FN的标签．

4. 提出的AutoNER模型仅需要词典，相对有监督方法有竞争力．

** 概述

*** 词典的构造

1. 实体词典

   1) 每一词条包含标准名与同义词．

   2) 每一词条包含实体类型．

2. 高质量短语

   通过挖掘高质量词条构造出潜在的“未知”类型实体．

*** 标签的构造

通过精确字符串匹配与最大化匹配数量来标注．

给定一个原始语料库，每一个token会被标注成以下三种中的一种：

1. 属于一种或多种已知类型．

2. 属于“未知”类型．

3. 非实体．

** 模型

*** Fuzzy-LSTM-CRF与修正版的IOBES

#+attr_html: :width 800px
#+attr_latex: :width 12cm
[[file:images/%E6%A8%A1%E5%9E%8B/2019-12-02_20-55-02_fuzzy-lstm-crf.png]]

为了支持多标签的序列，损失函数目标为极大化所有可能的标签序列的概率总和．当所有标签已知并唯一，等价于传统的CRF．

*** AutoNER与Tie or Break标注

#+attr_html: :width 800px
#+attr_latex: :width 12cm
[[file:images/Shang,_J.,_Liu,_L.,_Ren,_X.,_Gu,_X.,_Ren,_T.,_&_Han,_J._(2018):_Learning_named_entity_tagger_using_domain-specific_dictionary/2019-12-02_21-06-08_tie-or-break.png]]

Tie or Break标注主要考虑相邻token是否属于同一实体还是应该分隔：

1. Tie：如果两个token属于同一实体．

2. Unknown：至少一个token属于“未知”类型的高质量短语．

3. Break：其它情况．

任意两个Break之间形成一个token块，而每个token块与它所有的匹配类型关联．无类型关联的标记为None．

**** 动机

1. 远程监督的产生的实体边界可能会不正确，但其内部通常是正确的．

2. Unigram实体更可能为FP，但在Tie or Break的标注方式下，它们总是位于两个Break标签之间，所以不会引入错误标签的问题．

**** AutoNER流程

1. Entity Span Detection

   通过LSTM的输出与一个sigmoid层来预测Break标签．

2. Entity Type Prediction

   对齐LSTM的输出，对每一块的类型进行预测．由于每一块可能被标注有多种实体类型，所以损失函数使用了一种交叉熵的变种．

** 远程监督的优化

*** Corpus-Aware Dictionary Tailoring

针对FP：盲目使用全部词典可能会造成FP较多．这里只考虑至少出现过一次标准名的词条．

*** Unknown-Typed High-Quality Phrases

针对FN：为了减缓FN过多的问题，引入AutoPhrase抽取高质量词条添加到词典并作为潜在实体，并标注为“未知”类型．这些只有无法被这个扩展词典匹配的token块会被标记为非实体．

** 实验

只允许使用原始文本作为远程监督模型的输入．

1. AutoNER击败了上一个SOTA Distant-LSTM-CRF．

2. AutoNER性能可与有监督模型匹敌．

3. 使用[[*Corpus-Aware Dictionary Tailoring][修剪过的词典]]效果更好．

4. 添加了“未知”类型的[[*Unknown-Typed High-Quality Phrases][高质量短语]]效果也有提升．

5. 添加了gold training set之后（这里是怎么用的？），随着它数量的增多，远程监督甚至比有监督效果更好！（作者猜测远程监督可能强调实体的可匹配性，而标注数据有可能错过这些可匹配的实体．）

6. 有监督的模型需要一定数量的标注数据才达到AutoNER的相同效果．

* Wang, X., Zhang, Y., Li, Q., Ren, X., Shang, J., & Han, J. (2019). Distantly supervised biomedical named entity recognition with dictionary expansion
:PROPERTIES:
:Custom_ID: wang-2019-autobioner
:END:

** Related Works

- Fully supervised methods

  无法直接学到到新的实体类型．

- Distant supervision

  只能利用用户词典里的有限信息，尤其当词典并不完整的时候．这篇文章主要聚焦于如何通过实体集扩张处理词典不完整的问题．

- AutoBioNER

  + 不需要任何人工标注数据．

  + 依赖于一个不完整的实体字典．

  首先从用于生成候选实体的语料库与用户输入词典里挖掘统计特征用于训练数据标注．由于词典是不完整的，AutoBioNER进行自动实体集扩张用于语料库级别的实体识别和词典扩充．它将匹配到的实体作为正样本并结合上下文信息推断未能匹配的实体．扩充后的词典用于远程监督训练神经网络用于实体识别．在用户提供词典的情况下，能识别用户感兴趣的新实体．

** 贡献

- 提出了远程监督的框架AutoBioNER，能根据用户输入词典自动从大量语料库里识别生物医药的实体．

- 提出了一种实体集扩张的方法，结合自动短语挖掘与语料库级的新实体识别与词典扩充方法．

- 定性与定量分析词典扩充的重要性，并验证AutoBioNER的有效性．

** 框架

*** Phrase Mining and Dictionary Matching

**** Phrase Mining

使用AutoPhrase cite:shang-2017-autophrase 挖掘高质量短语．

**** Dictionary Matching

根据词典对挖掘到的短语进行类型匹配（在匹配过程里用到了Dictionary Tailoring cite:shang-2018-autoner ），由于词典不完整，未匹配的短语将在下一步利用匹配到的短语与未匹配到的短语的上下文相似性再次进行实体挖掘．

*** Entity Expansion

**** A Simple Way

直接利用候选短语与实体类型（即该类型下的实体集）的语义相似性．两个难点：

1. 每种实体类型集下的种子实体比较多样且稀疏．

   大的类别下可能还有很多小的类别．种子实体虽然数量庞大，但每个种子实体出现的频率相对较低，上下文信息较为稀疏，相互之间语义距离较远．（化学：药品、化学元素等．）

2. 通过短语挖掘出来的候选实体对实体集扩张来说具有噪声．有些可能的高频词实际上只是噪声（如\(p\)-value, mm, Hg等）．

**** AutoBioNER

#+attr_html: :width 800px
#+attr_latex: :width 12cm
[[file:images/Wang,_X.,_Zhang,_Y.,_Li,_Q.,_Ren,_X.,_Shang,_J.,_&_Han,_J._(2019)._Distantly_supervised_biomedical_named_entity_recognition_with_dictionary_expansion/2019-12-07_16-54-43_entity_expansion.png]]

首先对每种实体类型下的种子实体进行聚类，减少多样性与稀疏性．然后通过种子融合与特征融合减少集合扩张过程中的噪声．

The candidate entities are /ranked high for one entity type/ only if it satisfies both criteria:

1. It shares more context information with the seed entities of this type.
2. It shares context information with more seed entities of this type.

***** Semantic Closeness Scoring

语义相似性的度量：co-occurrence statistics与context features．本文在集合扩张过程同时使用两种特征．

Given a candidate phrase set \(P\) and a skip-gram (i.e. context feature) set \(C\), we define the similarity between each pair or phrase \(p\) and context \(c\) using the IF-IDF transformation cite:wang-2019-autobioner

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  f_{p, c} = \log (1 + X_{p, c})(\log |P| - \log \sum_{p' \in P} X_{p', c}), \nonumber
\end{eqnarray}
#+end_src

where \(X_{p', c}\) is the raw co-occurrence count between \(p\) and \(c\). 效果比PMI与BM25好 cite:shen-2017-setexpan．

Then the similarity between two phrases \(p_{1}\) and \(p_{2}\) under feature set \(C\) is defined as

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \text{sim}(p_{1}, p_{2}| C) = \frac{\sum_{c \in C} \min (f_{p_{1}, c}, f_{p_{2}, c})}{\sum_{c \in C} \max (f_{p_{1}, c}, f_{p_{2}, c})}. \nonumber
\end{eqnarray}
#+end_src

Given a seed entity set \(E\) and a skip-gram feature set \(C\), each candidate phrase \(p\) can be scored as

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \text{score}(p| E, C) = \frac{1}{|E|} \sum_{e \in E} \text{sim} (p, e| C). \nonumber
\end{eqnarray}
#+end_src

***** Seed Clustering

由于每个实体集非常多样，对某个候选短语不可能假设它离实体集里的每个元素都距离很近．所以利用\(k\)-Means根据word2vec embedding对每个实体集进行内部聚类．然后利用feature ensemble与seed ensemble为每个seed cluster进行最佳候选实体．

***** Seed Ensemble

For each \(E_{th}\), we sample \(N_{E}\) subsets \(E_{th}^{(j)} (j = 1, 2, \dots, N_{E})\). Each of the seed subsets contains \(M_{E}^{'} (M_{E}^{'} < |E_{th}|)\) features. 即对每个聚类后的种子集进行多次采样．

***** Feature Ensemble

For a context feature set \(C\), we first score each \(c \in C\) based on its accumulated strength with entities in \(E\) (i.e., \(\sum_{e \in E} f_{e, c}\)). The \(M_{C}\) skip-grams with the highest score will be selected, from which we sample \(N_{C}\) subsets \(C_{i} (i = 1, 2, \dots, N_{C})\). Each of the subsets contains \(M_{C}^{'} (M_{C}^{'} < M_{C})\) features.

For each \(C_{i}\) and \(E_{th}^{i}\), we obtain a ranking list of phrases according to their \(\text{score}(\cdot| C_{i}, E_{th}^{(j)})\). Suppose the rank of \(p\) in terms of \(\text{score}(\cdot| C_{i}, E_{th}^{(j)})\) is \(r_{pij}\), the mean reciprocal rank of \(p\) is

#+begin_src latex :results drawer :exports results
\begin{eqnarray}
  \text{MRR}(p| E_{th}) = \frac{1}{N_{C}N_{E}} \sum_{i = 1}^{N_{c}} \sum_{j = 1}^{N_{E}} \frac{1}{r_{pij}} \nonumber.
\end{eqnarray}
#+end_src

The phrases with \(\text{MRR}\) higher than a threshold \(\text{MRR}_{thrs}\) will be added into type \(E_{t}\).

*** Distant Training

参考[[#shang-2018-autoner][AutoNER]]．

** 实验

*** 主要问题

1. AutoBioNER与SOTA NER模型的对比．

2. 实体扩张的作用如何．

3. 由于AutoBioNER不需要任何人工标注数据，给定词典的时候对于新实体发现表现如何．

*** 结果

1. 纯词典匹配precision高，recall较低．

2. 进行词典扩充之后，recall有所提高．

3. 相对Fuzzy-LSTM-CRF，AutoNER在所有数据集上表现较好．

4. Dictionary-Match, Dictionary-Expansion和AutoNER可以看作AutoBioNER的框架的退化情况．通过实验证明框架里的每一模块对于提升效果都是必须的．我们要做的就是让recall的提升比precision的下降要更快．（从结果来看，AutoBioNER在precision下降的同时提高了recall．）

5. 对\(\text{MRR}\)指标选出来的实体进行了定性的分析，结果表明实体扩张选出来的实体准确率较高．

6. 对实体扩张过程里使用到的skip-grams与co-occurrence统计和直接使用word embedding两种方法的效果进行了比较．结果表明直接使用word embedding的效果不太好．因为embedding similarities只考虑语义但忽略了co-occurrence frequency．对于较低频的实体，它们的上下文比较有限，word2vec的质量可能不太好．而本文的模型同时考虑里semantics与frequency．

7. 定性分析表明对于新类型的实体，AutoBioNER通过词典的帮助能达到较高的recall．

* Sui, D., Chen, Y., Liu, K., Zhao, J., & Liu, S. (2019). Leverage lexical knowledge for Chinese named entity recognition via collaborative graph network

** Motivation

由于中文句子没有分隔符，所以做中文NER的一个直接想法是基于字符表示．但是，单词的信息对中文NER非常重要．

解决这个问题的常用做法是：

1. 基于词级别去做，但会面对错误传播的问题．
2. 联合训练，但花费大且标注方法可能比较多样．
3. 利用自动构建出来的在大型分割好的语料上pre-trained的词典．

这里单词信息的含义主要指：

- 单词边界信息：由单词本身提供．
- 单词语义信息：由pre-trained embedding提供．

利用词典知识的两个挑战是：

1. 怎样利用self-matched lexical words．

   #+begin_example
给定句子：希尔顿离开北京机场了．

比如单词“机场”和“北京机场”是字符“机”的self-matched words，但“离开”则不是．“北京机场”可以帮助预测“机”的正确标签是“I-LOC”而不是其它．
   #+end_example

2. 怎样直接利用最近邻上下文单词信息．

   #+begin_example
给定句子同上，“离开”是字符“顿”的最近邻上下文单词，它可以帮助预测“顿”的正确标签是“I-PER”，而不是“I-ORG”.
   #+end_example

** 回顾

- NER与中文NER上的改进，主要是分词信息等．

- GCN/GAT及它们在NLP上的应用及问题，已有模型都有别的依赖，存在误差传播的问题．

** 主要贡献

- 提出了能利用词典信息的CGN用于中文NER．

- 为了解决self-matched lexical words与nearest contextual lesical words的问题，提出了无需要其它外部工具辅助的3个word-character interactive graphs构建方法．图之间的交互能捕捉不同的词典知识．

- 在主流中文NER数据集上取得了SOTA．

** 图的构建

三个不同的word-character  interactive graphs分别是：

- Word-Character Containing Graph (C-Graph)

  帮助字符捕捉self-matched lexical words的边界和语义信息．

- Word-Character Transition Graph (T-Graph)

  帮助字符捕捉最近邻上下文词典词的语义信息．

- Word-Character Lattice Graph (L-Graph)

  网格结构可以捕捉到最近邻上下文词典词信息与self-matched lexical words的信息．

以上几个图具有相同听结点集，但采用各不相同的边集构建方式．

*** Word-Character Containing Graph

#+attr_html: :width 600px
#+attr_latex: :width 8cm
[[file:images/Sui,_D.,_Chen,_Y.,_Liu,_K.,_Zhao,_J.,_&_Liu,_S._(2019)._Leverage_lexical_knowledge_for_Chinese_named_entity_recognition_via_collaborative_graph_network/c-graph.png]]

每个匹配到的self-matched lexical word与它每一个字符相连．

*** Word-Character Transition Graph

#+attr_html: :width 600px
#+attr_latex: :width 8cm
[[file:images/Sui,_D.,_Chen,_Y.,_Liu,_K.,_Zhao,_J.,_&_Liu,_S._(2019)._Leverage_lexical_knowledge_for_Chinese_named_entity_recognition_via_collaborative_graph_network/t-graph.png]]

- 每个词典词或字符与它的前后字符相连．

- 匹配到的相邻词典词之间也要相连．

*** Word-Character Lattice Graph

#+attr_html: :width 600px
#+attr_latex: :width 8cm
[[file:images/Sui,_D.,_Chen,_Y.,_Liu,_K.,_Zhao,_J.,_&_Liu,_S._(2019)._Leverage_lexical_knowledge_for_Chinese_named_entity_recognition_via_collaborative_graph_network/l-graph.png]]


- 每个字符之间相连．

- 每个词典词与它的起始字符相连．

** 模型

#+attr_html: :width 800px
#+attr_latex: :width 10cm
[[file:images/Sui,_D.,_Chen,_Y.,_Liu,_K.,_Zhao,_J.,_&_Liu,_S._(2019)._Leverage_lexical_knowledge_for_Chinese_named_entity_recognition_via_collaborative_graph_network/cgn.png]]

* References

bibliographystyle:unsrt
bibliography:references.bib
